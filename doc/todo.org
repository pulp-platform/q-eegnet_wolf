* DONE Implement pooling before scaling
:LOGBOOK:
- State "DONE"       from "TODO"       [2020-01-23 Thu 17:05]
:END:
- Requires different scale function: [[../python_utils/convert_torch_format.py::def%20div_factor_batch_norm(input_scale,%20weight_scale,%20output_scale,%20bn_scale,%20bn_offset,][convert_torch_format.py]]
* DONE When scaling for batch norm, first add bias, and then divide by factor
- Requires different scale function: [[../python_utils/convert_torch_format.py::def%20div_factor_batch_norm(input_scale,%20weight_scale,%20output_scale,%20bn_scale,%20bn_offset,][convert_torch_format.py]]
- Different type in layer offset: [[../data/gen_net_header.py][gen_net_header.py]]
:LOGBOOK:
- State "DONE"       from "TODO"       [2020-01-19 Sun 18:48]
:END:
* DONE Pooling layer will cut off the remaining elements, which do not fit into a full kernel (at the end)
:LOGBOOK:
- State "DONE"       from "TODO"       [2020-02-06 Thu 13:57]
:END:
- Those values thus do not need to be computed
- Also, for implementation, we can to this pooling in higher precision, and include the factor in the scaling
* DONE add round constant to offset
:LOGBOOK:
- State "DONE"       from "TODO"       [2020-01-23 Thu 17:05]
:END:
